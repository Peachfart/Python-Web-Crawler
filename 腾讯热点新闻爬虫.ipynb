{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----第01页新闻爬取完成！-----\n",
      "-----第02页新闻爬取完成！-----\n",
      "-----第03页新闻爬取完成！-----\n",
      "-----第04页新闻爬取完成！-----\n",
      "-----第05页新闻爬取完成！-----\n",
      "-----第06页新闻爬取完成！-----\n",
      "-----第07页新闻爬取完成！-----\n",
      "-----第08页新闻爬取完成！-----\n",
      "-----第09页新闻爬取完成！-----\n",
      "-----第10页新闻爬取完成！-----\n",
      "-----所有页面爬取完成！-----\n",
      "任务完成！\n"
     ]
    }
   ],
   "source": [
    "import requests,json\n",
    "\n",
    "from time import sleep\n",
    "\n",
    "from random import random\n",
    "\n",
    "myheader = {'user-agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64;x64) AppleWebKit/537.36 (KHTML, like Gecko)Chrome/73.0.3683.103 Safari/537.36'}\n",
    "\n",
    "##一、将爬取到新闻数据保存为txt文件\n",
    "\n",
    "fout = open('hot_news.txt','w',encoding = 'utf-8')       #打开文本文件\n",
    "\n",
    "for offset in range(0,500,20):                           #构造网址遍历循环\n",
    "\n",
    "    url = 'https://i.news.qq.com/trpc.qqnews_web.kv_srv.kv_srv_http_proxy/list?sub_srv_id=24hours&srv_id=pc&offset='+str(offset)+'&limit=20&strategy=1&ext={%22pool%22:[%22top%22],%22is_filter%22:7,%22check_type%22:true}'\n",
    "\n",
    "    try:\n",
    "\n",
    "        r = requests.get(url,headers = myheader)         #请求json数据\n",
    "\n",
    "        json_data = json.loads(r.text)                   #解析json数据\n",
    "\n",
    "        news_list = json_data.get('data').get('list')    #提取新闻列表\n",
    "\n",
    "        for news in news_list:                           #遍历新闻列表\n",
    "\n",
    "            title = news.get('title')                    #提取标题\n",
    "\n",
    "            media = news.get('media_name')               #提取发布者\n",
    "\n",
    "            pub_time = news.get('publish_time')          #提取更新时间\n",
    "\n",
    "            cate = news.get('category_cn')               #提取类别\n",
    "\n",
    "            subcate = news.get('sub_category_cn')        #提取子类别\n",
    "\n",
    "            news_url = news.get('url')                   #提取新闻网址\n",
    "\n",
    "            print(title,media,pub_time,cate,subcate,news_url,file = fout,sep = '\\t')\n",
    "\n",
    "        sleep(random()*4)                                #随机暂停\n",
    "\n",
    "        print(f'-----第{offset//20+1:02}页新闻爬取完成！-----')\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        break\n",
    "\n",
    "fout.close()                                             #保存并关闭文件\n",
    "\n",
    "print('-----所有页面爬取完成！-----')\n",
    "\n",
    "##二、将txt文件保存为excl文件\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df= pd.read_csv('hot_news.txt',sep = '\\t',names=['标题','发布者','更新时间','类别','子类别','新闻网址'])\n",
    "\n",
    "df.to_excel('腾讯热点新闻信息.xlsx',index=False)\n",
    "\n",
    "print('任务完成！')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
